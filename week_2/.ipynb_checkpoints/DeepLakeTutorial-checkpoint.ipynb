{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5AYzB1IQAx-"
   },
   "source": [
    "# Deeplake Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKU8kmSs65xv"
   },
   "source": [
    "# **Step 1**: _Hello World_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrjGQON37lk2"
   },
   "source": [
    "## Installing Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pcfYcPu7KxY"
   },
   "source": [
    "Hub can be installed via `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oC_N5qOx6o0d"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip3 install hub\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4_rfJ_GVxLz"
   },
   "source": [
    "**By default, Hub does not install dependencies for audio, video, and google-cloud (GCS) support. They can be installed using:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwpEic3jV2nV"
   },
   "outputs": [],
   "source": [
    "#pip install hub[audio]  -> Audio support via miniaudio\n",
    "\n",
    "#pip install hub[video]  -> Video support via pyav\n",
    "\n",
    "#pip install hub[gcp]    -> GSS support via google-* dependencies\n",
    "\n",
    "#pip install hub[all]    -> Installs everything - audio, video and GCS support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0N-f2SYU7OjQ"
   },
   "source": [
    "## Fetching your first Hub dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aNFn7rZ7qxP"
   },
   "source": [
    "Begin by loading in [MNIST](https://en.wikipedia.org/wiki/MNIST_database), the hello world dataset of machine learning. \n",
    "\n",
    "First, load the `Dataset` by pointing to its storage location. Datasets hosted on the Activeloop Platform are typically identified by the namespace of the organization followed by the dataset name: `activeloop/mnist-train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "izccjS4k7NvX",
    "outputId": "a41c7355-fa7f-443b-ba49-d60a7ef90a25"
   },
   "outputs": [],
   "source": [
    "import hub\n",
    "\n",
    "dataset_path = 'hub://activeloop/mnist-train'\n",
    "ds = hub.load(dataset_path) # Returns a Hub Dataset but does not download data locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bR5n8yYg-0Wu"
   },
   "source": [
    "## Reading Samples From a Hub Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XdaAKaS-3NO"
   },
   "source": [
    "Data is not immediately read into memory because Hub operates [lazily](https://en.wikipedia.org/wiki/Lazy_evaluation). You can fetch data by calling the `.numpy()` method, which reads data into a NumPy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qpQeNoq-xfo"
   },
   "outputs": [],
   "source": [
    "# Indexing\n",
    "W = ds.images[0].numpy() # Fetch image return a NumPy array\n",
    "X = ds.labels[0].numpy(aslist=True) # Fetch label and store as list of NumPy array\n",
    "\n",
    "# Slicing\n",
    "Y = ds.images[0:100].numpy() # Fetch 100 images and return a NumPy array if possible\n",
    "                               # This method produces an exception if\n",
    "                               # the shape of the images is not equal\n",
    "Z = ds.labels[0:100].numpy(aslist=True) # Fetch 100 labels and store as list of \n",
    "                                           # NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNGHXfdKwJ7W",
    "outputId": "a4b485f8-54ad-4ca3-fec2-809c22c0c8b0"
   },
   "outputs": [],
   "source": [
    "print('X is {}'.format(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmi2w0_e_LtH"
   },
   "source": [
    "Congratulations, you've got Hub working on your local machine! ðŸ¤“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-DM6PKq_di2"
   },
   "source": [
    "# **Step 2**: _Creating Hub Datasets_\n",
    "*Creating and storing Hub Datasets manually.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEzK8LTe_gJW"
   },
   "source": [
    "Creating Hub datasets is simple, you have full control over connecting your source data (files, images, etc.) to specific tensors in the Hub Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGXGvKU1qsp1"
   },
   "source": [
    "## Manual Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQk29Mnhqn1V"
   },
   "source": [
    "Let's follow along with the example below to create our first dataset. First, download and unzip the small classification dataset below called the *animals dataset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDJRrlDP_DsW"
   },
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "from IPython.display import clear_output\n",
    "!wget https://github.com/activeloopai/examples/raw/main/colabs/starting_data/animals.zip\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIQf9cY6_vyn"
   },
   "outputs": [],
   "source": [
    "# Unzip to './animals' folder\n",
    "!unzip -qq /content/animals.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIz-MYImAfCg"
   },
   "source": [
    "The dataset has the following folder structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuhZZqVIAqj_"
   },
   "source": [
    "animals\n",
    "- cats\n",
    "  - image_1.jpg\n",
    "  - image_2.jpg\n",
    "- dogs\n",
    "  - image_3.jpg\n",
    "  - image_4.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Lez5uCJAto4"
   },
   "source": [
    "Now that you have the data, you can **create a Hub `Dataset`** and initialize its tensors. Running the following code will create a Hub dataset inside of the `./animals_hub` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qtzmT0iBNV23",
    "outputId": "5017cbbf-b744-4be7-88a4-5dbf63d5b986"
   },
   "outputs": [],
   "source": [
    "import hub\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "ds = hub.empty('./animals_hub') # Creates the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQ5yt0aaNeP5"
   },
   "source": [
    "Next, let's inspect the folder structure for the source dataset `'./animals'` to find the class names and the files that need to be uploaded to the Hub dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubGLkgG8Njbb"
   },
   "outputs": [],
   "source": [
    "# Find the class_names and list of files that need to be uploaded\n",
    "dataset_folder = './animals'\n",
    "\n",
    "class_names = os.listdir(dataset_folder)\n",
    "\n",
    "files_list = []\n",
    "for dirpath, dirnames, filenames in os.walk(dataset_folder):\n",
    "    for filename in filenames:\n",
    "        files_list.append(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtVSh0FnNmyI"
   },
   "source": [
    "Next, let's **create the dataset tensors and upload metadata**. Check out our page on [Storage Synchronization](https://docs.activeloop.ai/how-hub-works/storage-synchronization) for details about the `with` syntax below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6QDC6caNpiH",
    "outputId": "7f67985a-9ecd-478e-a320-93717079d492"
   },
   "outputs": [],
   "source": [
    "with ds:\n",
    "  # Create the tensors with names of your choice.\n",
    "  ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
    "  ds.create_tensor('labels', htype = 'class_label', class_names = class_names)\n",
    "\n",
    "  # Add arbitrary metadata - Optional\n",
    "  ds.info.update(description = 'My first Hub dataset')\n",
    "  ds.images.info.update(camera_type = 'SLR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TD-hCSBKBA_m"
   },
   "source": [
    "**Note:** Specifying `htype` and `dtype` is not required, but it is highly recommended in order to optimize performance, especially for large datasets. Use `dtype` to specify the numeric type of tensor data, and use `htype` to specify the underlying data structure. More information on `htype` can be found [here](https://api-docs.activeloop.ai/htypes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR4kLo6YBOhO"
   },
   "source": [
    "Finally, let's **populate the data** in the tensors.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QRAyS-HA-Fp",
    "outputId": "bd341eba-b8ea-4174-8e22-f2242b25ac6c"
   },
   "outputs": [],
   "source": [
    "with ds:\n",
    "    # Iterate through the files and append to hub dataset\n",
    "    for file in files_list:\n",
    "        label_text = os.path.basename(os.path.dirname(file))\n",
    "        label_num = class_names.index(label_text)\n",
    "        \n",
    "        #Append data to the tensors\n",
    "        ds.append({'images': hub.read(file), 'labels': np.uint32(label_num)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWqYzfI1DCPG"
   },
   "source": [
    "**Note:** `ds.append({'images': hub.read(path)})` is functionally equivalent to `ds.append({'images': PIL.Image.fromarray(path)})`. However, the `hub.read()` method is significantly faster because it does not decompress and recompress the image if the compression matches the `sample_compression` for that tensor. Further details are available in the next section.\n",
    "\n",
    "**Note:** In order to maintain proper indexing across tensors, `ds.append({...})` requires that you to append to all tensors in the dataset. If you wish to skip tensors during appending, please use `ds.append({...}, skip_ok = True)` or append to a single tensor using `ds.tensor_name.append(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzHVb521XSud"
   },
   "source": [
    "Check out the first image from this dataset. More details about Accessing Data are available in **Step 4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "OMG2oif0XSDZ",
    "outputId": "445cb305-2234-46b0-98b3-b374ce6bc230"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(ds.images[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK_wpkYsDdH2"
   },
   "source": [
    "## Creating Tensor Hierarchies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1btlOtBDDe4G"
   },
   "source": [
    "Often it's important to create tensors hierarchically, because information between tensors may be inherently coupledâ€”such as bounding boxes and their corresponding labels. Hierarchy can be created using tensor `groups`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICg3Z1z8CRGN",
    "outputId": "2e5d08c2-6a99-42c5-e120-b366f51da5ee"
   },
   "outputs": [],
   "source": [
    "ds = hub.empty('./groups_test') # Creates the dataset\n",
    "\n",
    "# Create tensor hierarchies\n",
    "ds.create_group('my_group')\n",
    "ds.my_group.create_tensor('my_tensor')\n",
    "\n",
    "# Alternatively, a group can us created using create_tensor with '/'\n",
    "ds.create_tensor('my_group_2/my_tensor') # Automatically creates the group 'my_group_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE-rWBCkpI9T"
   },
   "source": [
    "Tensors in groups are accessed via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78s3Oa_jpKXV",
    "outputId": "41a60262-10b9-490a-ceb9-f93cd43e7c41"
   },
   "outputs": [],
   "source": [
    "ds.my_group.my_tensor\n",
    "\n",
    "#OR\n",
    "\n",
    "ds['my_group/my_tensor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fhjWZ9hDvKe"
   },
   "source": [
    "For more detailed information regarding accessing datasets and their tensors, check out **Step 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46H4nEnZDv5m"
   },
   "source": [
    "# **Step 3**: _Understanding Compression_\n",
    "\n",
    "*Using compression to achieve optimal performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ajldDggEp8O"
   },
   "source": [
    "**Data in Hub can be stored in raw uncompressed format. However, compression is highly recommended for achieving optimal performance in terms of speed and storage.**\n",
    "\n",
    "\n",
    "Compression is specified separately for each tensor, and it can occur at the `sample` or `chunk` level. For example, when creating a tensor for storing images, you can choose the compression technique for the image samples using the `sample_compression` input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOw9hc0jDpQY",
    "outputId": "8ff22e06-9f3d-46dc-a52f-066e8fa7358d"
   },
   "outputs": [],
   "source": [
    "import hub\n",
    "\n",
    "# Set overwrite = True for re-runability\n",
    "ds = hub.empty('./compression_test', overwrite = True)\n",
    "\n",
    "ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv4ktXoCE2K2"
   },
   "source": [
    "In this example, every image added in subsequent `.append(...)` calls is compressed using the specified `sample_compression` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WaFBxrEE9GI"
   },
   "source": [
    "### **Choosing the Right Compression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM8VtZ98FCUu"
   },
   "source": [
    "There is no single answer for choosing the right compression, and the tradeoffs are described in detail in the next section. However, good rules of thumb are:\n",
    "\n",
    "\n",
    "\n",
    "1.   For data that has application-specific compressors (`image`, `audio`, `video`,...), choose the sample_compression technique that is native to the application such as `jpg`, `mp3`, `mp4`,...\n",
    "2.   For other data containing large samples (i.e. large arrays with >100 values), `lz4` is a generic compressor that works well in most applications. `lz4` can be used as a `sample_compression` or `chunk_compression`. In most cases, `sample_compression` is sufficient, but in theory, `chunk_compression` produces slightly smaller data.\n",
    "3.   For other data containing small samples (i.e. labels with <100 values), it is not necessary to use compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hotuAwslFbAu"
   },
   "source": [
    "### **Compression Tradeoffs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrWvN558v4xn"
   },
   "source": [
    "**Lossiness -** Certain compression techniques are lossy, meaning that there is irreversible information loss when compressing the data. Lossless compression is less important for data such as images and videos, but it is critical for label data such as numerical labels, binary masks, and segmentation data.\n",
    "\n",
    "\n",
    "**Memory -** Different compression techniques have substantially different memory footprints. For instance, png vs jpeg compression may result in a 10X difference in the size of a Hub dataset. \n",
    "\n",
    "\n",
    "**Runtime -** The primary variables affecting download and upload speeds for generating usable data are the network speed and available compute power for processing the data . In most cases, the network speed is the limiting factor. Therefore, the highest end-to-end throughput for non-local applications is achieved by maximizing compression and utilizing compute power to decompress/convert the data to formats that are consumed by deep learning models (i.e. arrays). \n",
    "\n",
    "\n",
    "**Upload Considerations -** When applicable, the highest uploads speeds can be achieved when the  `sample_compression` input matches the compression of the source data, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkJKv00UFexo"
   },
   "outputs": [],
   "source": [
    "# sample_compression is \"jpg\" and appended image is \"jpeg\"\n",
    "ds.create_tensor('images_jpg', htype = 'image', sample_compression = 'jpg')\n",
    "ds.images_jpg.append(hub.read('./animals/dogs/image_3.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LMsd3K9GJJ9"
   },
   "source": [
    "In this case, the input data is a `.jpg`, and the hub `sample_compression` is `jpg`. \n",
    "\n",
    "However, a mismatch between compression of the source data and sample_compression in Hub results in significantly slower upload speeds, because Hub must decompress the source data and recompress it using the specified `sample_compression` before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5MaC1lBwa3a"
   },
   "outputs": [],
   "source": [
    "# sample_compression is \"jpg\" and appended image is \"jpeg\"\n",
    "ds.create_tensor('images_png', htype = 'image', sample_compression = 'png')\n",
    "ds.images_png.append(hub.read('./animals/dogs/image_3.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXGCWxu7wjHX"
   },
   "source": [
    "**NOTE:** Due to the computational costs associated with decompressing and recompressing data, it is important that you consider the runtime implications of uploading source data that is compressed differently than the specified sample_compression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGo-E8Z8Ho6F"
   },
   "source": [
    "# **Step 4**: _Accessing Data_\n",
    "_Accessing and loading Hub Datasets._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8Mye_Z5Htut"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DI_D7flHvEN"
   },
   "source": [
    "Hub Datasets can be loaded and created in a variety of storage locations with minimal configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9dl3mfENulO"
   },
   "outputs": [],
   "source": [
    "import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sltdan65HmRN",
    "outputId": "1cf699cd-10f4-4367-aa62-e6434a4391c2"
   },
   "outputs": [],
   "source": [
    "# Local Filepath\n",
    "ds = hub.load('./animals_hub') # Dataset created in Step 2 in this Colab Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41FBvx25NWMN"
   },
   "outputs": [],
   "source": [
    "# S3\n",
    "# ds = hub.load('s3://my_dataset_bucket', creds={...})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuacdMOgNNmT",
    "outputId": "ec6c1bec-4101-4865-e44c-ff00596c5c3f"
   },
   "outputs": [],
   "source": [
    "# Public Dataset hosted by Activeloop\n",
    "## Activeloop Storage - See Step 6\n",
    "ds = hub.load('hub://activeloop/k49-train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocs18sNqNQfG"
   },
   "outputs": [],
   "source": [
    "# Dataset in another workspace on Activeloop Platform\n",
    "# ds = hub.load('hub://workspace_name/dataset_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZD60qFaAH2qg"
   },
   "source": [
    "**Note:** Since `ds = hub.dataset(path)` can be used to both create and load datasets, you may accidentally create a new dataset if there is a typo in the path you provided while intending to load a dataset. If that occurs, simply use `ds.delete()` to remove the unintended dataset permanently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Kb9q_ZqIARN"
   },
   "source": [
    "## Referencing Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq5WSI5LIClV"
   },
   "source": [
    "Hub allows you to reference specific tensors using keys or via the `.` notation outlined below. \n",
    "\n",
    "\n",
    "**Note:** data is still not loaded by these commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jr_ZEtBnN1Wp",
    "outputId": "457bdd57-423b-469d-89a5-0709ebe0af19"
   },
   "outputs": [],
   "source": [
    "ds = hub.dataset('hub://activeloop/k49-train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24trRqlLH0Tl",
    "outputId": "88c39d40-3abc-4150-9869-2e6fc1b85396"
   },
   "outputs": [],
   "source": [
    "### NO HIERARCHY ###\n",
    "ds.images # is equivalent to\n",
    "ds['images']\n",
    "\n",
    "ds.labels # is equivalent to\n",
    "ds['labels']\n",
    "\n",
    "### WITH HIERARCHY ###\n",
    "# ds.localization.boxes # is equivalent to\n",
    "# ds['localization/boxes']\n",
    "\n",
    "# ds.localization.labels # is equivalent to\n",
    "# ds['localization/labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjmnRLWHINXG"
   },
   "source": [
    "## Accessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js3jsmBHIPqu"
   },
   "source": [
    "Data within the tensors is loaded and accessed using the `.numpy()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QUWjQNGILWQ",
    "outputId": "72f71420-8577-41d8-e021-e5cbcd23dd61"
   },
   "outputs": [],
   "source": [
    "# Indexing\n",
    "ds = hub.dataset('hub://activeloop/k49-train')\n",
    "\n",
    "W = ds.images[0].numpy() # Fetch an image and return a NumPy array\n",
    "X = ds.labels[0].numpy(aslist=True) # Fetch a label and store it as a \n",
    "                                    # list of NumPy arrays\n",
    "\n",
    "# Slicing\n",
    "Y = ds.images[0:100].numpy() # Fetch 100 images and return a NumPy array\n",
    "                             # The method above produces an exception if \n",
    "                             # the images are not all the same size\n",
    "\n",
    "Z = ds.labels[0:100].numpy(aslist=True) # Fetch 100 labels and store \n",
    "                                        # them as a list of NumPy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DykgrsBEIfk1"
   },
   "source": [
    "**Note:** The `.numpy()` method will produce an exception if all samples in the requested tensor do not have a uniform shape. If that's the case, running `.numpy(aslist=True)` solves the problem by returning a list of NumPy arrays, where the indices of the list correspond to different samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K385Fpvmqc0l"
   },
   "source": [
    "#**Step 5**: *Visualizing Datasets*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSIK-TCAqqQF"
   },
   "source": [
    "One of Hub's core features is to enable users to visualize and interpret large amounts of data. Let's load the COCO dataset, which is one of the most popular datasets in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YRBC6ehqpgz",
    "outputId": "9fc58c6c-8481-4c40-ce07-c63ec67aad03"
   },
   "outputs": [],
   "source": [
    "import hub\n",
    "\n",
    "ds = hub.load('hub://activeloop/coco-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5TW4f4Zqzlw"
   },
   "source": [
    "The tensor layout for this dataset can be inspected using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YU10NNvNqz54"
   },
   "outputs": [],
   "source": [
    "ds.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StDTRjIJq3qI"
   },
   "source": [
    "The dataset can be [visualized in Platform](https://app.activeloop.ai/activeloop/coco-train), or using an iframe in a jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G3X22Tdq5tn"
   },
   "outputs": [],
   "source": [
    "ds.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L713rdfJtVKS"
   },
   "source": [
    "**Note:** Visualizing datasets in [Activeloop Platform](https://app.activeloop.ai/) will unlock more features and faster performance compared to visualization in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ul8Q0CK6rH50"
   },
   "source": [
    "##Visualizing your own datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQzmJazJrOaF"
   },
   "source": [
    "Any hub dataset can be visualized using the methods above as long as it follows the conventions necessary for the visualization engine to interpret and parse the data. These conventions [are explained here](https://docs.activeloop.ai/dataset-visualization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQipSo2OF_lB"
   },
   "source": [
    "# **Step 6**: _Using Activeloop Storage_ (optional -> needs account!)\n",
    "\n",
    "_Storing and loading datasets from Activeloop Platform Storage._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TJfXx2pgG7P"
   },
   "source": [
    "## Register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bA39G647GHX4"
   },
   "source": [
    "You can store your Hub Datasets with Activeloop by first creating an account in [Activeloop Platform](https://app.activeloop.ai/) or in the CLI using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCDC-5dmGFdJ"
   },
   "outputs": [],
   "source": [
    "!activeloop register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-nf5Sb-gMED"
   },
   "source": [
    "## Login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1iZpxtOGJ0N"
   },
   "source": [
    "In order for the Python API to authenticate with the Activeloop Platform, you should log in from the CLI using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0OUCCMGGLv0"
   },
   "outputs": [],
   "source": [
    "!activeloop login  # prompts for inputting username and password will follow ...\n",
    "\n",
    "# Alternatively, you can directly input your username and password in the same line:\n",
    "# !activeloop login -u my_username -p my_password\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvBxhaAYGNOi"
   },
   "source": [
    "You can then access or create Hub Datasets by passing the Activeloop Platform path to `hub.dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeL0a2zwGXeU"
   },
   "outputs": [],
   "source": [
    "import hub\n",
    "\n",
    "# platform_path = 'hub://workspace_name/dataset_name'\n",
    "#                 'hub://jane_smith/my_awesome_dataset'\n",
    "               \n",
    "ds = hub.dataset(platform_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huQQ1M8kGcyL"
   },
   "source": [
    "**Note**: When you create an account in Activeloop Platform, a default workspace is created that has the same name as your username. You are also able to create other workspaces that represent organizations, teams, or other collections of multiple users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUdVLQUGGnsA"
   },
   "source": [
    "Public datasets such as `hub://activeloop/mnist-train` can be accessed without logging in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgfj-ldqgZa_"
   },
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhguQ8IxgeBd"
   },
   "source": [
    "Once you have an Activeloop account, you can create tokens in [Activeloop Platform](https://app.activeloop.ai/) (Organization Details -> API Tokens) and pass them to python commands that require authentication using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxETFtMlgw0E"
   },
   "outputs": [],
   "source": [
    "#ds = hub.load(platform_path, token = 'xyz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVma__gxGq97"
   },
   "source": [
    "# **Step 7**: _Connecting Hub Datasets to ML Frameworks_\n",
    "\n",
    "_Connecting Hub Datasets to machine learning frameworks such as PyTorch and TensorFlow._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r-AkeJMGwxB"
   },
   "source": [
    "You can connect Hub Datasets to popular ML frameworks such as PyTorch and TensorFlow using minimal boilerplate code, and Hub takes care of the parallel processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bnr9ItdkGzDk"
   },
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKkrCv2NG1GG"
   },
   "source": [
    "You can train a model by creating a PyTorch DataLoader from a Hub Dataset using `ds.pytorch()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP3C2uoAGnNK"
   },
   "outputs": [],
   "source": [
    "import hub\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "ds = hub.dataset('hub://activeloop/cifar100-train') # Hub Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3J24ptPAyTw"
   },
   "source": [
    "The transform parameter in `ds.pytorch()` is a dictionary where the `key` is the tensor name and the `value` is the transformation function that should be applied to that tensor. If a specific tensor's data does not need to be returned, it should be omitted from the keys. If a tensor's data does not need to be modified during preprocessing, the transformation function is set as `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvqbqsCnA4P3"
   },
   "outputs": [],
   "source": [
    "tform = transforms.Compose([\n",
    "    transforms.ToPILImage(), # Must convert to PIL image for subsequent operations to run\n",
    "    transforms.RandomRotation(20), # Image augmentation\n",
    "    transforms.ToTensor(), # Must convert to pytorch tensor for subsequent operations to run\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "#PyTorch Dataloader\n",
    "dataloader= ds.pytorch(batch_size = 16, num_workers = 2, \n",
    "    transform = {'images': tform, 'labels': None}, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX_MIn_rA70v"
   },
   "source": [
    "You can iterate through the Hub DataLoader just like you would for a Pytorch DataLoader. Loading the first batch of data takes the longest time because the shuffle buffer is filled before any data is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuowobdbA96I"
   },
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    print(data)\n",
    "    break\n",
    "    # Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EQ2LUPydfPo"
   },
   "source": [
    "**Note:** Some datasets such as imagenet contain both grayscale and color images, which can cause errors when the transformed images are passed to the model. To convert only the grayscale images to color format, you can add this Torchvision transform to your pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Wi0K1aVdjMr"
   },
   "outputs": [],
   "source": [
    "# transforms.Lambda(lambda x: x.repeat(int(3/x.shape[0]), 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5bX92ZUG_2F"
   },
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeRUG-arHP1F"
   },
   "source": [
    "Similarly, you can convert a Hub Dataset to a TensorFlow Dataset via the `tf.Data` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1bma0HSHOAO"
   },
   "outputs": [],
   "source": [
    "ds # Hub Dataset object, to be used for training\n",
    "ds_tf = ds.tensorflow() # A TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guao84xTb4Zg"
   },
   "source": [
    "# **Step 8**: _Parallel Computing_\n",
    "\n",
    "_Running computations and processing data in parallel._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVcZ28epcKRc"
   },
   "source": [
    "Hub enables you to easily run computations in parallel and significantly accelerate your data processing workflows. This example primarily focuses on parallel dataset uploading, and other use cases such as dataset transformations can be found in [this tutorial](https://docs.activeloop.ai/tutorials/data-processing-using-parallel-computing).\n",
    "\n",
    "Parallel compute using Hub has two core elements: #1. defining a function or pipeline that will run in parallel and #2. evaluating it using the appropriate inputs and outputs. Let's start with #1 by defining a function that processes files and appends their data to the labels and images tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWNxzF1pcWxn"
   },
   "source": [
    "**Defining the parallel computing function**\n",
    "\n",
    "The first step for running parallel computations is to define a function that will run in parallel by decorating it using `@hub.compute`. In the example below, `file_to_hub` converts data from files into hub format, just like in **Step 2: Creating Hub Datasets Manually**. If you have not completed Step 2, please complete the section that downloads and unzips the *animals* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMjMF_-LcHtl"
   },
   "outputs": [],
   "source": [
    "import hub\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "@hub.compute\n",
    "def file_to_hub(file_name, sample_out, class_names):\n",
    "    ## First two arguments are always default arguments containing:\n",
    "    #     1st argument is an element of the input iterable (list, dataset, array,...)\n",
    "    #     2nd argument is a dataset sample\n",
    "    # Other arguments are optional\n",
    "    \n",
    "    # Find the label number corresponding to the file\n",
    "    label_text = os.path.basename(os.path.dirname(file_name))\n",
    "    label_num = class_names.index(label_text)\n",
    "    \n",
    "    # Append the label and image to the output sample\n",
    "    sample_out.labels.append(np.uint32(label_num))\n",
    "    sample_out.images.append(hub.read(file_name))\n",
    "    \n",
    "    return sample_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-ZhXH-pcgT8"
   },
   "source": [
    "In all functions decorated using `@hub.compute`, the first argument must be a single element of any input iterable that is being processed in parallel. In this case, that is a filename `file_name`, becuase `file_to_hub` reads image files and populates data in the dataset's tensors. \n",
    "\n",
    "The second argument is a dataset sample `sample_out`, which can be operated on using similar syntax to dataset objects, such as `sample_out.append(...)`, `sample_out.extend(...)`, etc.\n",
    "\n",
    "The function decorated using `@hub.compute` must return `sample_out`, which represents the data that is added or modified by that function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIUiNuQqchnH"
   },
   "source": [
    "**Executing the transform**\n",
    "\n",
    "To execute the transform, you must define the dataset that will be modified by the parallel computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZfEn1g_cno_"
   },
   "outputs": [],
   "source": [
    "ds = hub.empty('./animals_hub_transform') # Creates the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7FIReeLcpka"
   },
   "source": [
    "Next, you define the input iterable that describes the information that will be operated on in parallel. In this case, that is a list of files `files_list` from the animals dataset in Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CwypbTxcrx0"
   },
   "outputs": [],
   "source": [
    "# Find the class_names and list of files that need to be uploaded\n",
    "dataset_folder = './animals'\n",
    "\n",
    "class_names = os.listdir(dataset_folder)\n",
    "\n",
    "files_list = []\n",
    "for dirpath, dirnames, filenames in os.walk(dataset_folder):\n",
    "    for filename in filenames:\n",
    "        files_list.append(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IC-VRKVcuRI"
   },
   "source": [
    "You can now create the tensors for the dataset and **run the parallel computation** using the `.eval` syntax. Pass the optional input arguments to `file_to_hub`, and we skip the first two default arguments `file_name` and `sample_out`. \n",
    "\n",
    "The input iterable `files_list` and output dataset `ds` is passed to the `.eval` method as the first and second argument respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4H4Fug0cxJG"
   },
   "outputs": [],
   "source": [
    "with ds:\n",
    "    ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
    "    ds.create_tensor('labels', htype = 'class_label', class_names = class_names)\n",
    "    \n",
    "    file_to_hub(class_names=class_names).eval(files_list, ds, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfWc3_fkhr0W"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(ds.images[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xTj7kt0jrd3"
   },
   "source": [
    "Congrats! You just created a dataset using parallel computing! ðŸŽˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXRCphquSFs3"
   },
   "source": [
    "# **Step 9**: _Dataset Version Control_\n",
    "\n",
    "*Managing changes to your datasets using Version Control.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y_V53L8SCuB"
   },
   "source": [
    "Hub dataset version control allows you to manage changes to datasets with commands very similar to Git. It provides critical insights into how your data is evolving, and it works with datasets of any size!\n",
    "\n",
    "Let's check out how dataset version control works in Hub! If you haven't done so already, please download and unzip the *animals* dataset from **Step 2**. \n",
    "\n",
    "First let's create a hub dataset in the `./version_control_hub` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgEWowxySUDL"
   },
   "outputs": [],
   "source": [
    "import hub\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Set overwrite = True for re-runability\n",
    "ds = hub.dataset('./version_control_hub', overwrite = True)\n",
    "\n",
    "# Create a tensor and add an image\n",
    "with ds:\n",
    "    ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
    "    ds.images.append(hub.read('./animals/cats/image_1.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNLh-JE5pkS_"
   },
   "source": [
    "The first image in this dataset is a picture of a cat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4hVjQaVpksW"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(ds.images[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CEF-kjySdLp"
   },
   "source": [
    "##Commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joKq3VV0SdEW"
   },
   "source": [
    "To commit the data added above, simply run `ds.commit`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pj9uTZeSTGwT"
   },
   "outputs": [],
   "source": [
    "first_commit_id = ds.commit('Added image of a cat')\n",
    "\n",
    "print('Dataset in commit {} has {} samples'.format(first_commit_id, len(ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc2-MRmaSc4x"
   },
   "source": [
    "Next, let's add another image and commit the update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zArtG0phTZRv"
   },
   "outputs": [],
   "source": [
    "with ds:\n",
    "    ds.images.append(hub.read('./animals/dogs/image_3.jpg'))\n",
    "    \n",
    "second_commit_id = ds.commit('Added an image of a dog')\n",
    "\n",
    "print('Dataset in commit {} has {} samples'.format(second_commit_id, len(ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYjnY_1RTcjM"
   },
   "source": [
    "The second image in this dataset is a picture of a dog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbPu9JoFp0ap"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(ds.images[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWvgUH25Tj8V"
   },
   "source": [
    "##Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiqOb8POTkb4"
   },
   "source": [
    "The commit history starting from the current commit can be show using `ds.log`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQSxvzIcTuU-"
   },
   "outputs": [],
   "source": [
    "log = ds.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgefyAuATwi4"
   },
   "source": [
    "This command prints the log to the console and also assigns it to the specified variable log. The author of the commit is the username of the [Activeloop account](https://docs.activeloop.ai/getting-started/using-activeloop-storage) that logged in on the machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JRpqeYqV-oT"
   },
   "source": [
    "##Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TWcOT4RV-d4"
   },
   "source": [
    "Branching takes place by running the `ds.checkout` command with the parameter `create = True`. Let's create a new branch `dog_flipped`, flip the second image (dog), and create a new commit on that branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eY-CZmzrXr0X"
   },
   "outputs": [],
   "source": [
    "ds.checkout('dog_flipped', create = True)\n",
    "\n",
    "with ds:\n",
    "    ds.images[1] = np.transpose(ds.images[1], axes=[1,0,2])\n",
    "\n",
    "flipped_commit_id = ds.commit('Flipped the dog image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUUMXFKEXuIq"
   },
   "source": [
    "The dog image is now flipped and the log shows a commit on the `dog_flipped` branch as well as the previous commits on `main`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIP6V3VFqPKS"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(ds.images[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3-UgHZPX_0u"
   },
   "outputs": [],
   "source": [
    "ds.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCrKgp6FYDG9"
   },
   "source": [
    "##Checkout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07nHcIIiYFtW"
   },
   "source": [
    "A previous commit of branch can be checked out using `ds.checkout`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZe8iXjlYEdf"
   },
   "outputs": [],
   "source": [
    "ds.checkout('main')\n",
    "\n",
    "Image.fromarray(ds.images[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AZXuEVYYVHm"
   },
   "source": [
    "As expected, the dog image on `main` is not flipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmydIxas3XsV"
   },
   "source": [
    "## Diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTEuB-4C3a-B"
   },
   "source": [
    "Understanding changes between commits is critical for managing the evolution of datasets. Hub's `ds.diff` function enables users to determine the number of samples that were added, removed, or updated for each tensor. The function can be used in 3 ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhlPmK9E37Do"
   },
   "outputs": [],
   "source": [
    "ds.diff() # Diff between the current state and the last commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCa8-nlJ4Dxg"
   },
   "outputs": [],
   "source": [
    "ds.diff(first_commit_id) # Diff between the current state and a specific commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bj2Yez624Ecb"
   },
   "outputs": [],
   "source": [
    "ds.diff(second_commit_id, first_commit_id) # Diff between two specific commits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1GqH1JvYkNP"
   },
   "source": [
    "##HEAD Commit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbiRZ0eGiBrz"
   },
   "source": [
    "Unlike Git, Hub's version control does not have a staging area because changes to datasets are not stored locally before they are committed. All changes are automatically reflected in the dataset's permanent storage (local or cloud). **Therefore, any changes to a dataset are automatically stored in a HEAD commit on the current branch**. This means that the uncommitted changes do not appear on other branches. Let's see how this works:\n",
    "\n",
    "You should currently be on the `main` branch, which has 2 samples. Let's adds another image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwuzyJUViZC6"
   },
   "outputs": [],
   "source": [
    "print('Dataset on {} branch has {} samples'.format('main', len(ds)))\n",
    "\n",
    "with ds:\n",
    "    ds.images.append(hub.read('./animals/dogs/image_4.jpg'))\n",
    "    \n",
    "print('After updating, the HEAD commit on {} branch has {} samples'.format('main', len(ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3qePpVFqkG9"
   },
   "source": [
    "The 3rd sample is also an image of a dog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDfKKuhLqlMM"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(ds.images[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4brOnBdyiq6p"
   },
   "source": [
    "Next, if you checkout `dog_flipped` branch, the dataset contains 2 samples, which is sample count from when that branch was created. Therefore, the additional uncommitted third sample that was added to the `main` branch above is not reflected when other branches or commits are checked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvG-X9VqipM3"
   },
   "outputs": [],
   "source": [
    "ds.checkout('dog_flipped')\n",
    "\n",
    "print('Dataset in {} branch has {} samples'.format('dog_flipped', len(ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aoAeA7vixsC"
   },
   "source": [
    "Finally, when checking our the `main` branch again, the prior uncommitted changes and visible and they are stored in the `HEAD` commit on `main`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DnXiwTmi6G9"
   },
   "outputs": [],
   "source": [
    "ds.checkout('main')\n",
    "\n",
    "print('Dataset in {} branch has {} samples'.format('main', len(ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztVUV_BDqyHR"
   },
   "source": [
    "The dataset now contains 3 samples and the uncommitted dog image is visible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ci0IHCP9q0In"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(ds.images[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uinXs4r1i7Zz"
   },
   "source": [
    "##Merge - Coming Soon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQOGilvkjG2c"
   },
   "source": [
    "Merging is a critical feature for collaborating on datasets, and Activeloop is currently working on an implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz15ukH5jiIm"
   },
   "source": [
    "Congrats! You just are now an expert in dataset version control!ðŸŽ“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBM1DKntaXwS"
   },
   "source": [
    "# **Step 10:** *Dataset Filtering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8K5WREdaf--"
   },
   "source": [
    "Filtering and querying is an important aspect of data engineering because it enables users to focus on subsets of their datasets in order to obtain important insights, perform quality control, and train models on parts of their data. \n",
    "\n",
    "Hub enables you to perform queries using user-defined functions or Hub's Pythonic query language, all of which can be parallelized using our simple multi-processing API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp9i4Flsai0p"
   },
   "source": [
    "## Filtering with user-defined-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtwDPVwmapjL"
   },
   "source": [
    "The first step for querying using UDFs is to define a function that returns a boolean depending on whether an input sample in a dataset meets the user-defined condition. In this example, we define a function that returns `True` if the labels for a tensor are in the desired labels_list. If there are inputs to the filtering function other than `sample_in`, it must be decorated with `@hub.compute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lWTjNbUamyp"
   },
   "outputs": [],
   "source": [
    "@hub.compute\n",
    "def filter_labels(sample_in, labels_list, class_names):\n",
    "    text_label = class_names[sample_in.labels.numpy()[0]]\n",
    "    \n",
    "    return text_label in labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwITkEcmvdNo"
   },
   "source": [
    "Let's load a dataset and specify the `labels_list` that we want to filter for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLb_JgfxbYGA"
   },
   "outputs": [],
   "source": [
    "import hub\n",
    "from PIL import Image\n",
    "\n",
    "ds = hub.load('hub://activeloop/cifar10-test')\n",
    "\n",
    "labels_list = ['automobile', 'ship'] # Desired labels for filtering\n",
    "class_names = ds.labels.info.class_names # Mapping from numeric to text labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccAa1QuZazC-"
   },
   "source": [
    "The filtering function is executed using the `ds.filter()` command below, and it returns a virtual view of the dataset (`dataset_view`) that only contains the indices that met the filtering condition. Just like in the Parallel Computing API, the `sample_in` parameter does not need to be passed into the filter function when evaluating it, and multi-processing can be specified using the `scheduler` and `num_workers` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mq4e5gRIbZ74"
   },
   "outputs": [],
   "source": [
    "ds_view = ds.filter(filter_labels(labels_list, class_names), scheduler = 'threaded', num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2DowEHfbb2m"
   },
   "source": [
    "The data in the returned `ds_view` can be accessed just like a regular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FCxLH0Nbec8"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(ds_view.images[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ibjQKAEbs9F"
   },
   "source": [
    "**Note:** in most cases, multi-processing is not necessary for queries that involve simple data such as labels or bounding boxes. However, multi-processing significantly accelerates queries that must load rich data types such as images and videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OjmRFFVb1pn"
   },
   "source": [
    "## Filtering using our pythonic query language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgIOy-aJcOBF"
   },
   "source": [
    "Queries can also be executed using hub's Pythonic query language. This UX is primarily intended for use in [Activeloop Platform](https://app.activeloop.ai/), but it can also be applied programmatically in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YS4CD0Wncb99"
   },
   "outputs": [],
   "source": [
    "ds_view = ds.filter(\"labels == 'automobile' or labels == 'automobile'\", scheduler = 'threaded', num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtaZboftcemF"
   },
   "source": [
    "Tensors can be referred to by name, the language supports common logical operations (`in, ==, !=, >, <, >=, <=`), and numpy-like operators and indexing can be applied such as `'images.min > 5'`, `'images.shape[2]==1'`, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEi-DxIOcpHp"
   },
   "source": [
    "Congrats! You just learned to filter data with hub! ðŸŽˆ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lKU8kmSs65xv",
    "G-DM6PKq_di2",
    "46H4nEnZDv5m",
    "JGo-E8Z8Ho6F",
    "K385Fpvmqc0l",
    "NQipSo2OF_lB",
    "LVma__gxGq97",
    "guao84xTb4Zg",
    "iXRCphquSFs3",
    "vBM1DKntaXwS"
   ],
   "name": " Getting Started with Hub 2.0",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
