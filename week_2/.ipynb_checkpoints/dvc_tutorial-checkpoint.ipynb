{
 "cells": [
  {
   "attachments": {
    "dvc.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Version Control\n",
    "\n",
    "Tutorial based on [DVC use cases](https://dvc.org/doc/use-cases/data-and-model-files-versioning), also see [DVC features](https://dvc.org/features).\n",
    "\n",
    "![dvc.png](attachment:dvc.png)\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to play with an actual Machine Learning scenario. It explores the NLP problem of predicting tags for a given StackOverflow\n",
    "question. For example, we want one classifier which can predict a post that is about the Python language by tagging it python.\n",
    "\n",
    "This tutorial has been made adopting [DVC get-Started tutorial](https://dvc.org/doc/get-started/agenda) and full credit goes to DVC team for making that.\n",
    "A github repo for get-starter tutorial can be found [here](https://github.com/iterative/example-get-started)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing DVC\n",
    "\n",
    "Installing DVC is very easy. There are mainly three recommended ways:\n",
    "- pip\n",
    "- OS-specific package managers\n",
    "- HomeBrew(for apple users)\n",
    "\n",
    "\n",
    "We are going to install with `pip- Python package manger`. For other installation\n",
    "methods checkout [here](https://dvc.org/doc/get-started/install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Installing DVC\n",
    "! pip install dvc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Checking out DVC installation\n",
    "! dvc -h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising NLP Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir get-started && cd get-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "a = Path.cwd() / \"get-started\"\n",
    "os.chdir(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Initialising git in our folder\n",
    "! git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DVC initialization in a repository directory to create the DVC meta files and directories\n",
    "! dvc init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# configuring git for user account\n",
    "! git config --global user.name \"NAME\" #Replace with your github username\n",
    "! git config --global user.email \"MAIL\" #Replace with your email id\n",
    "# commit the initialised git files\n",
    "! git commit -m \"initialize DVC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring DVC remotes\n",
    "\n",
    "\n",
    "A DVC remote is used to share your ML models and datasets with others. The various types of remotes DVC currently supports is:\n",
    "https://dvc.org/doc/get-started/configure\n",
    "- `local` - Local directory\n",
    "- `s3` - Amazon Simple Storage Service\n",
    "- `gs` - Google Cloud Storage\n",
    "- `azure` - Azure Blob Storage\n",
    "- `ssh` - Secure Shell\n",
    "- `hdfs` - The Hadoop Distributed File System\n",
    "- `http` - Support for HTTP and HTTPS protocolbucks\n",
    "\n",
    "> Note we are using remote as a local directory as storage. **It's usually recommended to use Cloud storage services as DVC remote.**\n",
    "\n",
    "[More information](https://dvc.org/doc/get-started/configure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dvc remote add -d myremote /tmp/dvc-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    " ! git commit .dvc/config -m \"initialize DVC local remote\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Download the data\n",
    "! mkdir data/\n",
    "!  dvc get https://github.com/iterative/dataset-registry \\\n",
    "        get-started/data.xml -o data/data.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add file(directory) to DVC\n",
    "! dvc add data/data.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# add DVC files to git and update gitignore\n",
    "! git add data/.gitignore data/data.xml.dvc\n",
    "! git commit -m \"add source data to DVC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[more information](https://dvc.org/doc/get-started/add-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  push them from your repository to the default remote storage*:\n",
    "! dvc push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Data\n",
    "\n",
    "Now since we pushed our data, we are going to do the opposite of push ie `pull` similar to git analogy.\n",
    "An easy way to test it is by removing currently downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -f data/data.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now your data returns back to repositary\n",
    "! dvc pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incase just to retrieve single dataset or file\n",
    "! dvc pull data/data.xml.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting with code\n",
    "\n",
    "For providing full Machine Learning reproducibility. It is important to connect code with Datasets which are being reproducible by\n",
    "using commands like `dvc add/push/pull`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# run these commands to get the sample code:\n",
    "! wget wget https://code.dvc.org/get-started/code.zip\n",
    "! unzip code.zip\n",
    "! rm -f code.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having installed the `src/prepare.py` script in your repo, the following command\n",
    "transforms it into a reproducible\n",
    "[stage](https://dvc.org/doc/user-guide/dvc-files-and-directories) for the ML pipeline we're\n",
    "building (described in detail [in the documentation](https://dvc.org/doc/get-started/example-pipeline))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stages are run using dvc run [command] and options among which we use:\n",
    "\n",
    "- d for dependency: specify an input file\n",
    "- o for output: specify an output file ignored by git and tracked by dvc\n",
    "- M for metric: specify an output file tracked by git\n",
    "- f for file: specify the name of the dvc file.\n",
    "- command: a bash command, mostly a python script invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to create  folder data/prepared with files train.tsv and test.tsv\n",
    "! dvc run -f prepare.dvc \\\n",
    "          -d src/prepare.py -d data/data.xml \\\n",
    "          -o data/prepared \\\n",
    "          python src/prepare.py data/data.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!  git add data/.gitignore prepare.dvc\n",
    "!  git commit -m \"add data preparation stage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dvc push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Using `dvc run` multiple times, and specifying outputs of a command (stage) as dependencies in another one, we can describe a sequence of commands that gets to a desired result.\n",
    "This is what we call a data pipeline or computational graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a second stage (after prepare.dvc, created in the previous chapter) to perform feature extraction\n",
    "! dvc run -f featurize.dvc \\\n",
    "          -d src/featurization.py -d data/prepared/ \\\n",
    "          -o data/features \\\n",
    "           python src/featurization.py data/prepared data/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A third stage for training the model\n",
    "! dvc run -f train.dvc \\\n",
    "          -d src/train.py -d data/features \\\n",
    "          -o model.pkl \\\n",
    "          python src/train.py data/features model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git add data/.gitignore .gitignore featurize.dvc train.dvc\n",
    "git commit -m \"add featurization and train steps to the pipeline\"\n",
    "dvc push\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "! dvc pipeline show --ascii train.dvc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "The last stage we would like to add to our pipeline is its the evaluation. Data science is a metric-driven R&D-like process and `dvc metrics` along with DVC metric \n",
    "files provide a framework to capture and compare experiments performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`evaluate.py` calculates AUC value using the test data set. It reads features from the `features/test.pkl` file and produces a DVC metric file - `auc.metric`. It is a special DVC output file type, in this case it's just a plain text file with a single number inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dvc run -f evaluate.dvc \\\n",
    "          -d src/evaluate.py -d model.pkl -d data/features \\\n",
    "          -M auc.metric \\\n",
    "          python src/evaluate.py model.pkl \\\n",
    "                 data/features auc.metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please, refer to the [dvc metrics](https://dvc.org/doc/commands-reference/metrics) command documentation to see more available options and details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git add evaluate.dvc auc.metric\n",
    "git commit -m \"add evaluation step to the pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag as a checkpoint to cpmpare further experiments\n",
    "! git tag -a \"baseline-experiment\" -m \"baseline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Data science process is inherently iterative and R&D like - data scientist may try many different approaches, different hyper-parameter values and \"fail\" \n",
    "many times before the required level of a metric is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are modifying our feature extraction of our files. Inorder to use `bigrams`. We are increasing no of features and n_gram_range in our file `src/featurization.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "%%writefile src/featurization.py\n",
    "import os\n",
    "import sys\n",
    "import errno\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "if len(sys.argv) != 3 and len(sys.argv) != 5:\n",
    "    sys.stderr.write('Arguments error. Usage:\\n')\n",
    "    sys.stderr.write('\\tpython featurization.py data-dir-path features-dir-path\\n')\n",
    "    sys.exit(1)\n",
    "\n",
    "train_input = os.path.join(sys.argv[1], 'train.tsv')\n",
    "test_input = os.path.join(sys.argv[1], 'test.tsv')\n",
    "train_output = os.path.join(sys.argv[2], 'train.pkl')\n",
    "test_output = os.path.join(sys.argv[2], 'test.pkl')\n",
    "\n",
    "try:\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding('utf-8')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def get_df(data):\n",
    "    df = pd.read_csv(\n",
    "        data,\n",
    "        encoding='utf-8',\n",
    "        header=None,\n",
    "        delimiter='\\t',\n",
    "        names=['id', 'label', 'text']\n",
    "    )\n",
    "    sys.stderr.write('The input data frame {} size is {}\\n'.format(data, df.shape))\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_matrix(df, matrix, output):\n",
    "    id_matrix = sparse.csr_matrix(df.id.astype(np.int64)).T\n",
    "    label_matrix = sparse.csr_matrix(df.label.astype(np.int64)).T\n",
    "\n",
    "    result = sparse.hstack([id_matrix, label_matrix, matrix], format='csr')\n",
    "\n",
    "    msg = 'The output matrix {} size is {} and data type is {}\\n'\n",
    "    sys.stderr.write(msg.format(output, result.shape, result.dtype))\n",
    "\n",
    "    with open(output, 'wb') as fd:\n",
    "        pickle.dump(result, fd, pickle.HIGHEST_PROTOCOL)\n",
    "    pass\n",
    "\n",
    "\n",
    "mkdir_p(sys.argv[2])\n",
    "\n",
    "# Generate train feature matrix\n",
    "df_train = get_df(train_input)\n",
    "train_words = np.array(df_train.text.str.lower().values.astype('U'))\n",
    "\n",
    "bag_of_words = CountVectorizer(stop_words='english',\n",
    "                               max_features=5000,\n",
    "                              ngram_range=(1, 2),)\n",
    "bag_of_words.fit(train_words)\n",
    "train_words_binary_matrix = bag_of_words.transform(train_words)\n",
    "tfidf = TfidfTransformer(smooth_idf=False)\n",
    "tfidf.fit(train_words_binary_matrix)\n",
    "train_words_tfidf_matrix = tfidf.transform(train_words_binary_matrix)\n",
    "\n",
    "save_matrix(df_train, train_words_tfidf_matrix, train_output)\n",
    "\n",
    "# Generate test feature matrix\n",
    "df_test = get_df(test_input)\n",
    "test_words = np.array(df_test.text.str.lower().values.astype('U'))\n",
    "test_words_binary_matrix = bag_of_words.transform(test_words)\n",
    "test_words_tfidf_matrix = tfidf.transform(test_words_binary_matrix)\n",
    "\n",
    "save_matrix(df_test, test_words_tfidf_matrix, test_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce\n",
    "\n",
    "We described our first pipeline. Basically, we created a number of DVC-file. Each file describes a single stage we need to run (a pipeline) towards a final result.\n",
    "Each depends on some data (either source data files or some intermediate results from another DVC-file file) and code files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DVC Repro here \n",
    "! dvc repro train.dvc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git commit -a -m \"bigram model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git checkout baseline-experiment\n",
    "! dvc checkout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Expermiments\n",
    "\n",
    "DVC makes it easy to iterate on your project using Git commits with tags or Git branches. It provides a way to try different ideas, keep track of them, \n",
    "switch back and forth. To find the best performing experiment or track the progress, a special metric output type is supported in \n",
    "DVC (described in one of the previous steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git checkout master\n",
    "dvc checkout\n",
    "dvc repro evaluate.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git commit -a -m \"evaluate bigram model\"\n",
    "git tag -a \"bigram-experiment\" -m \"bigrams\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dvc metrics show -T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get older Data files\n",
    "\n",
    "The answer is the `dvc checkout` command, and we already touched briefly the process of switching between different data versions in the Experiments step of this get started guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git checkout baseline-experiment train.dvc\n",
    "! dvc checkout train.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git checkout baseline-experiment\n",
    "! dvc checkout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------\n",
    "\n",
    "- https://dvc.org/doc/get-started\n",
    "- https://medium.com/qonto-engineering/using-dvc-to-create-an-efficient-version-control-system-for-data-projects-96efd94355fe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
