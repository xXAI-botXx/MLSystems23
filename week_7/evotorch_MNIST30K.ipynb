{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA_6KkMpXDL5"
      },
      "source": [
        "## Training a CNN on MNIST using evotorch\n",
        "\n",
        "This example demonstrates the application of the `evotorch.neuroevolution.SupervisedNE` `Problem` class to training a CNN on MNIST. This example follows set-up described in the recent DeepMind paper [1].\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision \n",
        "!pip install evotorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpmGMY31XOFg",
        "outputId": "9bdcbe18-b2db-4b42-d824-47ad6a30d277"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evotorch\n",
            "  Downloading evotorch-0.4.1-py3-none-any.whl (271 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.2/271.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cma (from evotorch)\n",
            "  Downloading cma-3.3.0-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting functorch (from evotorch)\n",
            "  Downloading functorch-2.0.0-py2.py3-none-any.whl (2.1 kB)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from evotorch) (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from evotorch) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from evotorch) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evotorch) (23.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evotorch) (1.5.3)\n",
            "Collecting ray>=1.0 (from evotorch)\n",
            "  Downloading ray-2.4.0-cp310-cp310-manylinux2014_x86_64.whl (58.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from evotorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from ray>=1.0->evotorch) (23.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=1.0->evotorch) (8.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray>=1.0->evotorch) (3.12.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray>=1.0->evotorch) (4.3.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=1.0->evotorch) (1.0.5)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray>=1.0->evotorch) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray>=1.0->evotorch) (6.0)\n",
            "Collecting aiosignal (from ray>=1.0->evotorch)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist (from ray>=1.0->evotorch)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray>=1.0->evotorch) (2.27.1)\n",
            "Collecting virtualenv<20.21.1,>=20.0.24 (from ray>=1.0->evotorch)\n",
            "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<=1.51.3,>=1.42.0 (from ray>=1.0->evotorch)\n",
            "  Downloading grpcio-1.51.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->evotorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->evotorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->evotorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->evotorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->evotorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->evotorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->evotorch) (16.0.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->evotorch) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->evotorch) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->evotorch) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->evotorch) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->evotorch) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->evotorch) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->evotorch) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->evotorch) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->evotorch) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evotorch) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->evotorch) (1.16.0)\n",
            "Collecting distlib<1,>=0.3.6 (from virtualenv<20.21.1,>=20.0.24->ray>=1.0->evotorch)\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray>=1.0->evotorch) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->evotorch) (2.1.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=1.0->evotorch) (0.19.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray>=1.0->evotorch) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray>=1.0->evotorch) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->ray>=1.0->evotorch) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray>=1.0->evotorch) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->evotorch) (1.3.0)\n",
            "Installing collected packages: distlib, virtualenv, grpcio, frozenlist, cma, aiosignal, ray, functorch, evotorch\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.54.0\n",
            "    Uninstalling grpcio-1.54.0:\n",
            "      Successfully uninstalled grpcio-1.54.0\n",
            "Successfully installed aiosignal-1.3.1 cma-3.3.0 distlib-0.3.6 evotorch-0.4.1 frozenlist-1.3.3 functorch-2.0.0 grpcio-1.51.3 ray-2.4.0 virtualenv-20.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJYWNQYYXDMk"
      },
      "source": [
        "### Setting up the `Problem` class\n",
        "First we will define the model. For this example, we will use the 'MNIST-30k' model from the paper, which is defined below. Note that Table 1 has a typo; the second convolution should have a 5x5 kernel, rather than a 2x2 kernel. This gives the number of parameters the authors listed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQj-6nETXDMw",
        "outputId": "299e4885-204f-45c5-9a67-aa1a8caf1213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:evotorch:The logger is already configured. The default configuration will not be applied. Call `set_default_logger_config` with `override=True` to override the current configuration.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network has 28938 parameters\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from evotorch.neuroevolution.net import count_parameters\n",
        "\n",
        "class MNIST30K(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        # The first convolution uses a 5x5 kernel and has 16 filters\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size = 5, stride = 1, padding=2)\n",
        "        # Then max pooling is applied with a kernel size of 2\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "        # The second convolution uses a 5x5 kernel and has 32 filters\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size = 5, stride = 1, padding = 2)\n",
        "        # Another max pooling is applied with a kernel size of 2\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "        \n",
        "        # Apply layer normalization after the second pool\n",
        "        self.norm = nn.LayerNorm(1568, elementwise_affine=False)\n",
        "        \n",
        "        # A final linear layer maps outputs to the 10 target classes\n",
        "        self.out = nn.Linear(1568, 10)\n",
        "        \n",
        "        # All activations are ReLU\n",
        "        self.act = nn.ReLU()\n",
        "        \n",
        "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
        "        # Apply the first conv + pool\n",
        "        data = self.pool1(self.act(self.conv1(data)))\n",
        "        # Apply the second conv + pool\n",
        "        data = self.pool2(self.act(self.conv2(data)))\n",
        "        \n",
        "        # Apply layer norm\n",
        "        data = self.norm(data.flatten(start_dim = 1))\n",
        "        \n",
        "        # Flatten and apply the output linear layer\n",
        "        data = self.out(data)\n",
        "        \n",
        "        return data\n",
        "        \n",
        "network = MNIST30K()\n",
        "print(f'Network has {count_parameters(network)} parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpwM7yjJXDND"
      },
      "source": [
        "Now lets pull the dataset (to use with standard transforms). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9avbTi3XDNJ",
        "outputId": "6ab13329-0cc9-4882-f372-de13a9db73bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 135356988.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 49402811.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 38151590.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3975485.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transform)\n",
        "test_dataset = datasets.MNIST('../data', train=False,\n",
        "                   transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYzXLo11XDNM"
      },
      "source": [
        "Now we are ready to create a custom problem class. The below is configured to use 4 actors, and divide the available GPUs between them. You can scale this up to dozens or even hundreds of CPUs and GPUs on a `ray` cluster simply by modifying the `num_actors` parameter. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUzoP41MXDNN",
        "outputId": "546ddb77-611b-4297-b19e-00963a66c773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-23 16:08:49,426\tINFO worker.py:1625 -- Started a local Ray instance.\n"
          ]
        }
      ],
      "source": [
        "from evotorch.neuroevolution import SupervisedNE\n",
        "\n",
        "mnist_problem = SupervisedNE(\n",
        "    train_dataset,  # Using the dataset specified earlier\n",
        "    MNIST30K,  # Training the MNIST30K module designed earlier\n",
        "    nn.CrossEntropyLoss(),  # Minimizing CrossEntropyLoss\n",
        "    minibatch_size = 1024,  # With a minibatch size of 1024\n",
        "    common_minibatch = True,  # Always using the same minibatch across all solutions on an actor\n",
        "    num_actors = 4,  # The total number of CPUs used\n",
        "    num_gpus_per_actor = 'max',  # Dividing all available GPUs between the 4 actors\n",
        "    subbatch_size = 50,  # Evaluating solutions in sub-batches of size 50 ensures we won't run out of GPU memory for individual workers\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpEbkZuiXDNX"
      },
      "source": [
        "## Training\n",
        "Now we can set up the searcher.\n",
        "\n",
        "In the paper, they used SNES with, effectively, default parameters, and standard deviation 1. The authors achieved 98%+ with only a population size of 1k, but this value can be pushed further. Note that by using the `distributed = True` keyword argument, we obtain semi-updates from the individual actors which are averaged.\n",
        "\n",
        "In our example, we use PGPE with a population size of 3200. Hyperparameter configuration can be seen below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [],
        "id": "XO_qIYSyXDNd"
      },
      "outputs": [],
      "source": [
        "from evotorch.algorithms import PGPE\n",
        "\n",
        "searcher = PGPE(\n",
        "    mnist_problem,\n",
        "    radius_init=2.25, # Initial radius of the search distribution\n",
        "    center_learning_rate=1e-2, # Learning rate used by adam optimizer\n",
        "    stdev_learning_rate=0.1, # Learning rate for the standard deviation\n",
        "    popsize=3200,\n",
        "    distributed=True, # Gradients are computed locally at actors and averaged\n",
        "    optimizer=\"adam\", # Using the adam optimizer\n",
        "    ranking_method=None, # No rank-based fitness shaping is used\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gKMtMrQXDNe"
      },
      "source": [
        "Let's create some loggers. We'll run evolution for quite a long time, so it's worth reducing the log frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [],
        "id": "bD84AuORXDNh"
      },
      "outputs": [],
      "source": [
        "from evotorch.logging import StdOutLogger, PandasLogger\n",
        "stdout_logger = StdOutLogger(searcher, interval = 1)\n",
        "pandas_logger = PandasLogger(searcher, interval = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXXH7nT9XDNt"
      },
      "source": [
        "Running evolution for 400 generations (note that in the paper, it was 10k generations)..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcYB-DkqXDNu",
        "outputId": "b2915464-c438-4aad-bdef-31062edc44fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(EvaluationActor pid=1269)\u001b[0m /usr/local/lib/python3.10/dist-packages/evotorch/core.py:3425: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "\u001b[2m\u001b[36m(EvaluationActor pid=1269)\u001b[0m   shares_storage = self._data.storage().data_ptr() == source._data.storage().data_ptr()\n",
            "\u001b[2m\u001b[36m(EvaluationActor pid=1382)\u001b[0m /usr/local/lib/python3.10/dist-packages/evotorch/core.py:3425: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[2m\u001b[36m(EvaluationActor pid=1382)\u001b[0m   shares_storage = self._data.storage().data_ptr() == source._data.storage().data_ptr()\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     iter : 1\n",
            "mean_eval : 2.4175324626266956\n",
            "\n",
            "     iter : 2\n",
            "mean_eval : 2.433001846075058\n",
            "\n"
          ]
        }
      ],
      "source": [
        "searcher.run(400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efPi3J43XDNw"
      },
      "source": [
        "We can visualize the progress:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YaBmFPgOXDN3"
      },
      "outputs": [],
      "source": [
        "pandas_logger.to_dataframe().mean_eval.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmzaBTKpXDN_"
      },
      "source": [
        "And of course, it is worth while to measure the test performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "bjehkizVXDOA"
      },
      "outputs": [],
      "source": [
        "#net = mnist_problem.parameterize_net(searcher.status['center']).cpu()\n",
        "net = mnist_problem.make_net(searcher.status[\"center\"]).cpu()\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "net.eval()\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 256, shuffle = False)\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = net(data)\n",
        "        test_loss += loss(output, target).item() * data.shape[0]\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "print('Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "test_loss, correct, len(test_loader.dataset),\n",
        "100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "OCHM9S9DXDOK"
      },
      "outputs": [],
      "source": [
        "mnist_problem.kill_actors()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exs9bEzVXDOQ"
      },
      "source": [
        "#### References\n",
        "\n",
        "[1] Lenc, Karel, et al. [\"Non-differentiable supervised learning with evolution strategies and hybrid methods.\"](https://www.deepmind.com/publications/non-differentiable-supervised-learning-with-evolution-strategies-and-hybrid-methods) arXiv preprint arXiv:1906.03139 (2019).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}